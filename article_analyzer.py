#!/usr/bin/env python3
"""æ–‡ç« åˆ†ææ¨¡çµ„ - åˆ†æPTTæ–‡ç« ä¸­çš„æŠ•è³‡æ¨™çš„å’Œç­–ç•¥."""

import re
import json
from typing import List, Dict, Optional, Tuple
import httpx
from datetime import datetime
from loguru import logger
from database import db_manager
from models import PTTArticle
from sqlalchemy import desc


class ArticleAnalyzer:
    """æ–‡ç« åˆ†æå™¨."""
    
    def __init__(self):
        self.stock_patterns = {
            # ç¾è‚¡ä»£ç¢¼ (1-5å€‹å­—æ¯)
            'us_stocks': r'\b([A-Z]{1,5})\b',
            # å°è‚¡ä»£ç¢¼ (4ä½æ•¸å­—)
            'tw_stocks': r'\b([0-9]{4})\b',
            # æ¸¯è‚¡ä»£ç¢¼ (4-5ä½æ•¸å­—)
            'hk_stocks': r'\b([0-9]{4,5})\b'
        }
        
        # æŠ•è³‡ç­–ç•¥é—œéµè©
        self.strategy_keywords = {
            'buy_signals': ['è²·', 'å¤š', 'çœ‹å¤š', 'æ¨è–¦', 'å»ºè­°', 'å»ºå€‰', 'åŠ å€‰', 'æŒæœ‰'],
            'sell_signals': ['è³£', 'ç©º', 'çœ‹ç©º', 'æ¸›å€‰', 'å‡ºå ´', 'ç²åˆ©äº†çµ'],
            'risk_keywords': ['é¢¨éšª', 'æ³¨æ„', 'å°å¿ƒ', 'è¬¹æ…', 'æ³¢å‹•', 'ä¸ç¢ºå®š'],
            'sector_keywords': ['åŠå°é«”', 'AI', 'é›»å‹•è»Š', 'æ–°èƒ½æº', 'ç”ŸæŠ€', 'é‡‘è', 'åœ°ç”¢']
        }
    
    def analyze_article(self, article_id: str) -> Dict:
        """åˆ†æå–®ç¯‡æ–‡ç« ."""
        with db_manager.get_session() as session:
            article = session.query(PTTArticle).filter(
                PTTArticle.article_id == article_id
            ).first()
            
            if not article:
                return {"error": "æ–‡ç« ä¸å­˜åœ¨"}
            
            return self._analyze_content(article)
    
    def analyze_article_by_url(self, url: str) -> Dict:
        """æ ¹æ“šURLåˆ†ææ–‡ç« ."""
        with db_manager.get_session() as session:
            article = session.query(PTTArticle).filter(
                PTTArticle.url == url
            ).first()
            
            if not article:
                return {"error": "æ–‡ç« ä¸å­˜åœ¨"}
            
            return self._analyze_content(article)
    
    def _analyze_content(self, article: PTTArticle) -> Dict:
        """åˆ†ææ–‡ç« å…§å®¹ - ç°¡åŒ–è¼¸å‡ºæ ¼å¼."""
        content = article.content
        title = article.title
        
        # è‹¥å•Ÿç”¨ Ollamaï¼Œå„ªå…ˆèµ° LLM åˆ†æè·¯å¾‘
        try:
            from config import settings
            if getattr(settings, "enable_ollama", False):
                llm = self._analyze_with_ollama(content=content, author=article.author, url=article.url, date=article.publish_time)
                if isinstance(llm, dict) and llm.get("recommended_stocks"):
                    return llm
        except Exception as e:
            logger.warning(f"Ollama analysis failed or disabled, fallback to rules. Reason: {e}")
        
        # æå–è‚¡ç¥¨ä»£ç¢¼
        stocks = self._extract_stocks(content)
        
        # åˆ†ææŠ•è³‡ç­–ç•¥
        strategy = self._analyze_strategy(content)
        
        # åˆ†ææƒ…æ„Ÿå‚¾å‘
        sentiment = self._analyze_sentiment(content)
        
        # åˆ†æç”¢æ¥­é¡åˆ¥
        sectors = self._analyze_sectors(content)
        
        # åˆ†ææŠ•è³‡å»ºè­°
        recommendations = self._extract_recommendations(content)
        
        # åˆ†æé¢¨éšªæç¤º
        risks = self._extract_risks(content)
        
        # ç°¡åŒ–è¼¸å‡ºæ ¼å¼
        return {
            "author": article.author,
            "date": article.publish_time.strftime('%Y-%m-%d') if article.publish_time else 'N/A',
            "url": article.url,
            "recommended_stocks": list(stocks.get('mentioned_stocks', []))[:5],  # æœ€å¤š5å€‹æ¨è–¦æ¨™çš„
            "reason": self._generate_simple_reason(stocks, strategy, sentiment, sectors, risks)
        }

    def _analyze_with_ollama(self, content: str, author: str, url: str, date: Optional[datetime]) -> Dict:
        """ä½¿ç”¨æœ¬åœ° Ollama é€²è¡Œåˆ†æï¼Œè¿”å›æ—¢å®š JSON çµæ§‹ã€‚"""
        from config import settings
        base = settings.ollama_base_url.rstrip("/")
        model = settings.ollama_model
        prompt = (
            "è«‹é–±è®€ä»¥ä¸‹PTTæ–‡ç« å…§å®¹ï¼Œè¼¸å‡ºJSONï¼Œæ¬„ä½ç‚º: author, date(YYYY-MM-DD), url, "
            "recommended_stocks(æœ€å¤š5å€‹ï¼Œå°è‚¡ä»£ç¢¼è«‹ç”¨4ä½æ•¸å­—æˆ–æ˜ç¢ºä»£ç¢¼å­—ä¸²), reason(ç²¾ç°¡æ¢åˆ—èªªæ˜æ¨è–¦ä¾æ“š)ã€‚\n\n"
            f"ä½œè€…: {author}\nURL: {url}\næ—¥æœŸ: {(date.strftime('%Y-%m-%d') if isinstance(date, datetime) else 'N/A')}\n\n"
            f"å…§æ–‡:\n{content}\n\n"
            "åƒ…è¼¸å‡ºJSONï¼Œä¸è¦é¡å¤–æ•˜è¿°ã€‚"
        )
        payload = {"model": model, "prompt": prompt, "stream": False}
        try:
            with httpx.Client(timeout=30) as client:
                resp = client.post(f"{base}/api/generate", json=payload)
                resp.raise_for_status()
                data = resp.json()
                # Ollama å›å‚³å½¢å¦‚ {"response": "...æ–‡æœ¬..."}
                raw = data.get("response", "").strip()
                # å˜—è©¦è§£æJSON
                result = json.loads(raw)
                # æ­£è¦åŒ–æ¬„ä½
                return {
                    "author": result.get("author", author),
                    "date": result.get("date", (date.strftime('%Y-%m-%d') if isinstance(date, datetime) else 'N/A')),
                    "url": result.get("url", url),
                    "recommended_stocks": result.get("recommended_stocks", [])[:5],
                    "reason": result.get("reason", "")
                }
        except Exception as e:
            logger.warning(f"Ollama call/parse error: {e}")
            return {}
    
    def _generate_simple_reason(self, stocks: Dict, strategy: Dict, sentiment: Dict, sectors: List, risks: List) -> str:
        """ç”Ÿæˆç°¡åŒ–çš„æ¨è–¦åŸå› ."""
        reasons = []
        
        # åŸºæ–¼è‚¡ç¥¨ä»£ç¢¼æ•¸é‡
        stock_count = len(stocks.get('mentioned_stocks', []))
        if stock_count > 0:
            if stock_count == 1:
                reasons.append(f"æ¨è–¦æ¨™çš„: {stocks['mentioned_stocks'][0]}")
            else:
                reasons.append(f"æ¨è–¦{stock_count}å€‹æ¨™çš„")
        
        # åŸºæ–¼ç­–ç•¥
        buy_signals = len(strategy.get('buy_signals', []))
        sell_signals = len(strategy.get('sell_signals', []))
        
        if buy_signals > sell_signals:
            reasons.append("çœ‹å¤šè¨Šè™Ÿ")
        elif sell_signals > buy_signals:
            reasons.append("çœ‹ç©ºè¨Šè™Ÿ")
        
        # åŸºæ–¼æƒ…æ„Ÿ
        positive = sentiment.get('positive', 0)
        negative = sentiment.get('negative', 0)
        
        if positive > negative:
            reasons.append("æ­£é¢æƒ…ç·’")
        elif negative > positive:
            reasons.append("è² é¢æƒ…ç·’")
        
        # åŸºæ–¼ç”¢æ¥­
        if sectors:
            reasons.append(f"é—œæ³¨{sectors[0]}ç”¢æ¥­")
        
        # åŸºæ–¼é¢¨éšª
        if risks:
            reasons.append("æ³¨æ„é¢¨éšª")
        
        # å¦‚æœæ²’æœ‰å…·é«”åŸå› ï¼Œå˜—è©¦å¾å…§å®¹ä¸­æå–é—œéµä¿¡æ¯
        if not reasons:
            reasons.append("æŠ€è¡“åˆ†æ")
        
        return "ã€".join(reasons) if reasons else "æŠ€è¡“åˆ†æ"
    
    def _extract_stocks(self, content: str) -> Dict:
        """æå–è‚¡ç¥¨ä»£ç¢¼."""
        stocks = {
            "us_stocks": [],
            "tw_stocks": [],
            "hk_stocks": [],
            "mentioned_stocks": []
        }
        
        # æ”¹é€²çš„å°è‚¡ä»£ç¢¼æå– - 4ä½æ•¸å­—ï¼Œæ’é™¤å¹´ä»½å’Œå¸¸è¦‹æ•¸å­—
        tw_pattern = r'\b([0-9]{4})\b'
        tw_matches = re.findall(tw_pattern, content)
        
        # éæ¿¾æ‰å¹´ä»½å’Œå¸¸è¦‹æ•¸å­—
        filtered_tw = []
        for match in tw_matches:
            # æ’é™¤å¹´ä»½ (2020-2030)
            if not (2020 <= int(match) <= 2030):
                # æ’é™¤å¸¸è¦‹çš„æ•¸å­—åºåˆ—å’ŒæŠ€è¡“è¦æ ¼
                if not match in ['0000', '1111', '2222', '3333', '4444', '5555', '6666', '7777', '8888', '9999', '3120', '2500', '1230', '1070', '1090', '1330', '1575', '8000', '5800', '3000', '1000', '2000', '10000', '8550', '1400', '2800', '5600']:
                    filtered_tw.append(match)
        
        stocks["tw_stocks"] = list(set(filtered_tw))
        
        # æå–ç¾è‚¡ä»£ç¢¼ (1-5å€‹å­—æ¯)
        us_pattern = r'\b([A-Z]{1,5})\b'
        us_matches = re.findall(us_pattern, content)
        # éæ¿¾æ‰å¸¸è¦‹çš„è‹±æ–‡å–®è©å’ŒæŠ€è¡“è¡“èª
        common_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'HER', 'WAS', 'ONE', 'OUR', 'HAD', 'BY', 'WORD', 'BUT', 'WHAT', 'SOME', 'WE', 'IT', 'IS', 'OR', 'HAD', 'THE', 'OF', 'TO', 'AND', 'A', 'IN', 'IS', 'IT', 'YOU', 'THAT', 'HE', 'WAS', 'FOR', 'ON', 'ARE', 'AS', 'WITH', 'HIS', 'THEY', 'I', 'AT', 'BE', 'THIS', 'HAVE', 'FROM', 'OR', 'ONE', 'HAD', 'BY', 'WORD', 'BUT', 'NOT', 'WHAT', 'ALL', 'WERE', 'WE', 'WHEN', 'YOUR', 'CAN', 'SAID', 'THERE', 'EACH', 'WHICH', 'SHE', 'DO', 'HOW', 'THEIR', 'IF', 'WILL', 'UP', 'OTHER', 'ABOUT', 'OUT', 'MANY', 'THEN', 'THEM', 'THESE', 'SO', 'SOME', 'HER', 'WOULD', 'MAKE', 'LIKE', 'INTO', 'HIM', 'TIME', 'HAS', 'TWO', 'MORE', 'GO', 'NO', 'WAY', 'COULD', 'MY', 'THAN', 'FIRST', 'BEEN', 'CALL', 'WHO', 'ITS', 'NOW', 'FIND', 'LONG', 'DOWN', 'DAY', 'DID', 'GET', 'COME', 'MADE', 'MAY', 'PART', 'ATH', 'Q4', 'Q1', 'Q2', 'EPS', 'PE', 'DJI', 'AI', 'COBRA', 'RKLB', 'ONDS', 'AVAV', 'RCAT', 'PDYN', 'TOP', 'M', 'W', 'X', 'Y', 'Z'}
        stocks["us_stocks"] = list(set([match for match in us_matches if match not in common_words and len(match) <= 5]))
        
        # æå–æ¸¯è‚¡ä»£ç¢¼ (4-5ä½æ•¸å­—)
        hk_pattern = r'\b([0-9]{4,5})\b'
        hk_matches = re.findall(hk_pattern, content)
        # éæ¿¾æ‰æŠ€è¡“è¦æ ¼æ•¸å­—
        filtered_hk = []
        for match in hk_matches:
            if len(match) >= 4 and not (2020 <= int(match) <= 2030):
                if not match in ['3120', '2500', '1230', '1070', '1090', '1330', '1575', '8000', '5800', '3000', '1000', '2000', '10000', '8550', '1400', '2800', '5600']:
                    filtered_hk.append(match)
        stocks["hk_stocks"] = list(set(filtered_hk))
        
        # åˆä½µæ‰€æœ‰è‚¡ç¥¨
        all_stocks = stocks["us_stocks"] + stocks["tw_stocks"] + stocks["hk_stocks"]
        stocks["mentioned_stocks"] = list(set(all_stocks))
        
        return stocks
    
    def _analyze_strategy(self, content: str) -> Dict:
        """åˆ†ææŠ•è³‡ç­–ç•¥."""
        strategy = {
            "buy_signals": [],
            "sell_signals": [],
            "strategy_type": "unknown"
        }
        
        # æª¢æŸ¥è²·å…¥ä¿¡è™Ÿ
        for keyword in self.strategy_keywords['buy_signals']:
            if keyword in content:
                strategy["buy_signals"].append(keyword)
        
        # æª¢æŸ¥è³£å‡ºä¿¡è™Ÿ
        for keyword in self.strategy_keywords['sell_signals']:
            if keyword in content:
                strategy["sell_signals"].append(keyword)
        
        # åˆ¤æ–·ç­–ç•¥é¡å‹
        if strategy["buy_signals"] and not strategy["sell_signals"]:
            strategy["strategy_type"] = "bullish"
        elif strategy["sell_signals"] and not strategy["buy_signals"]:
            strategy["strategy_type"] = "bearish"
        elif strategy["buy_signals"] and strategy["sell_signals"]:
            strategy["strategy_type"] = "mixed"
        
        return strategy
    
    def _analyze_sentiment(self, content: str) -> Dict:
        """åˆ†ææƒ…æ„Ÿå‚¾å‘."""
        positive_words = ['çœ‹å¥½', 'æ¨‚è§€', 'æˆé•·', 'æ©Ÿæœƒ', 'æ½›åŠ›', 'åˆ©å¤š', 'ä¸Šæ¼²', 'çªç ´']
        negative_words = ['çœ‹å£', 'æ‚²è§€', 'è¡°é€€', 'é¢¨éšª', 'åˆ©ç©º', 'ä¸‹è·Œ', 'ç ´åº•']
        
        positive_count = sum(1 for word in positive_words if word in content)
        negative_count = sum(1 for word in negative_words if word in content)
        
        if positive_count > negative_count:
            sentiment = "positive"
        elif negative_count > positive_count:
            sentiment = "negative"
        else:
            sentiment = "neutral"
        
        return {
            "sentiment": sentiment,
            "positive_score": positive_count,
            "negative_score": negative_count,
            "confidence": abs(positive_count - negative_count) / max(positive_count + negative_count, 1)
        }
    
    def _analyze_sectors(self, content: str) -> List[str]:
        """åˆ†æç”¢æ¥­é¡åˆ¥."""
        sectors = []
        for sector in self.strategy_keywords['sector_keywords']:
            if sector in content:
                sectors.append(sector)
        
        # é¡å¤–çš„ç”¢æ¥­è­˜åˆ¥
        if 'åŠå°é«”' in content or 'æ™¶ç‰‡' in content or 'AMD' in content or 'NV' in content:
            sectors.append('åŠå°é«”')
        if 'AI' in content or 'äººå·¥æ™ºæ…§' in content:
            sectors.append('AI')
        if 'é›»å‹•è»Š' in content or 'ç‰¹æ–¯æ‹‰' in content or 'TSLA' in content:
            sectors.append('é›»å‹•è»Š')
        if 'æ–°èƒ½æº' in content or 'å¤ªé™½èƒ½' in content or 'é¢¨é›»' in content:
            sectors.append('æ–°èƒ½æº')
        if 'ç”ŸæŠ€' in content or 'é†«ç™‚' in content or 'è£½è—¥' in content:
            sectors.append('ç”ŸæŠ€')
        if 'è¡›æ˜Ÿ' in content or 'å¤ªç©º' in content or 'SpaceX' in content:
            sectors.append('å¤ªç©ºç§‘æŠ€')
        if 'æ ¸èƒ½' in content or 'æ ¸é›»' in content:
            sectors.append('æ ¸èƒ½')
        
        return list(set(sectors))
    
    def _extract_recommendations(self, content: str) -> List[Dict]:
        """æå–æŠ•è³‡å»ºè­°."""
        recommendations = []
        
        # å°‹æ‰¾åƒ¹æ ¼å»ºè­°
        price_patterns = [
            r'(\d+(?:\.\d+)?)\s*å…ƒ',
            r'(\d+(?:\.\d+)?)\s*ç¾å…ƒ',
            r'(\d+(?:\.\d+)?)\s*USD'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                recommendations.append({
                    "type": "price_target",
                    "value": float(match),
                    "context": "åƒ¹æ ¼ç›®æ¨™"
                })
        
        # å°‹æ‰¾æ™‚é–“å»ºè­°
        time_patterns = [
            r'(\d+)\s*å¹´',
            r'(\d+)\s*æœˆ',
            r'(\d+)\s*å­£'
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                recommendations.append({
                    "type": "time_horizon",
                    "value": int(match),
                    "context": "æŠ•è³‡æœŸé–“"
                })
        
        return recommendations
    
    def _extract_risks(self, content: str) -> List[str]:
        """æå–é¢¨éšªæç¤º."""
        risks = []
        
        risk_indicators = [
            'é¢¨éšª', 'æ³¨æ„', 'å°å¿ƒ', 'è¬¹æ…', 'æ³¢å‹•', 'ä¸ç¢ºå®š', 'å¯èƒ½', 'æˆ–è¨±',
            'ä½†æ˜¯', 'ä¸é', 'ç„¶è€Œ', 'é›–ç„¶', 'å„˜ç®¡'
        ]
        
        for indicator in risk_indicators:
            if indicator in content:
                risks.append(indicator)
        
        return list(set(risks))
    
    def _extract_investment_thesis(self, content: str) -> str:
        """æå–æŠ•è³‡è«–è¿°."""
        # å°‹æ‰¾é—œéµè«–è¿°æ®µè½
        thesis_indicators = ['å› ç‚º', 'æ‰€ä»¥', 'å› æ­¤', 'ç”±æ–¼', 'åŸºæ–¼', 'æ ¹æ“š']
        
        for indicator in thesis_indicators:
            if indicator in content:
                # æå–åŒ…å«è©²æŒ‡ç¤ºè©çš„å¥å­
                sentences = content.split('ã€‚')
                for sentence in sentences:
                    if indicator in sentence:
                        return sentence.strip()
        
        return "æœªæ‰¾åˆ°æ˜ç¢ºæŠ•è³‡è«–è¿°"
    
    def _extract_price_targets(self, content: str) -> List[Dict]:
        """æå–åƒ¹æ ¼ç›®æ¨™."""
        price_targets = []
        
        # å°‹æ‰¾åƒ¹æ ¼ç›®æ¨™æ¨¡å¼
        patterns = [
            r'ç›®æ¨™åƒ¹\s*(\d+(?:\.\d+)?)',
            r'çœ‹åˆ°\s*(\d+(?:\.\d+)?)',
            r'ä¸Šçœ‹\s*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*å…ƒ'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                price_targets.append({
                    "price": float(match),
                    "context": "åƒ¹æ ¼ç›®æ¨™"
                })
        
        return price_targets
    
    def _analyze_time_horizon(self, content: str) -> str:
        """åˆ†ææŠ•è³‡æ™‚é–“æ¡†æ¶."""
        if any(word in content for word in ['çŸ­æœŸ', 'è¿‘æœŸ', 'é€™é€±', 'é€™å€‹æœˆ']):
            return "çŸ­æœŸ"
        elif any(word in content for word in ['ä¸­æœŸ', 'å¹¾å€‹æœˆ', 'åŠå¹´']):
            return "ä¸­æœŸ"
        elif any(word in content for word in ['é•·æœŸ', 'å¹¾å¹´', 'é•·æœŸæŒæœ‰']):
            return "é•·æœŸ"
        else:
            return "æœªæ˜ç¢º"
    
    def batch_analyze_articles(self, author: str = None, limit: int = 10) -> List[Dict]:
        """æ‰¹é‡åˆ†ææ–‡ç« ."""
        with db_manager.get_session() as session:
            query = session.query(PTTArticle)
            
            if author:
                query = query.filter(PTTArticle.author == author)
            
            articles = query.order_by(desc(PTTArticle.publish_time)).limit(limit).all()
            
            results = []
            for article in articles:
                try:
                    analysis = self._analyze_content(article)
                    results.append(analysis)
                except Exception as e:
                    logger.error(f"åˆ†ææ–‡ç«  {article.article_id} å¤±æ•—: {e}")
                    continue
            
            return results
    
    def get_author_investment_profile(self, author: str) -> Dict:
        """åˆ†æä½œè€…æŠ•è³‡åå¥½."""
        with db_manager.get_session() as session:
            articles = session.query(PTTArticle).filter(
                PTTArticle.author == author
            ).order_by(desc(PTTArticle.publish_time)).all()
            
            if not articles:
                return {"error": "æ‰¾ä¸åˆ°è©²ä½œè€…çš„æ–‡ç« "}
            
            # çµ±è¨ˆåˆ†æ
            all_stocks = []
            all_sectors = []
            all_sentiments = []
            
            for article in articles:
                analysis = self._analyze_content(article)
                all_stocks.extend(analysis["analysis"]["stocks"]["mentioned_stocks"])
                all_sectors.extend(analysis["analysis"]["sectors"])
                all_sentiments.append(analysis["analysis"]["sentiment"]["sentiment"])
            
            # è¨ˆç®—çµ±è¨ˆ
            stock_frequency = {}
            for stock in all_stocks:
                stock_frequency[stock] = stock_frequency.get(stock, 0) + 1
            
            sector_frequency = {}
            for sector in all_sectors:
                sector_frequency[sector] = sector_frequency.get(sector, 0) + 1
            
            sentiment_frequency = {}
            for sentiment in all_sentiments:
                sentiment_frequency[sentiment] = sentiment_frequency.get(sentiment, 0) + 1
            
            return {
                "author": author,
                "total_articles": len(articles),
                "favorite_stocks": sorted(stock_frequency.items(), key=lambda x: x[1], reverse=True)[:10],
                "favorite_sectors": sorted(sector_frequency.items(), key=lambda x: x[1], reverse=True)[:5],
                "sentiment_distribution": sentiment_frequency,
                "investment_style": self._determine_investment_style(all_sectors, all_sentiments)
            }
    
    def _determine_investment_style(self, sectors: List[str], sentiments: List[str]) -> str:
        """åˆ¤æ–·æŠ•è³‡é¢¨æ ¼."""
        if 'åŠå°é«”' in sectors and 'AI' in sectors:
            return "ç§‘æŠ€æˆé•·å‹"
        elif 'æ–°èƒ½æº' in sectors and 'æ ¸èƒ½' in sectors:
            return "èƒ½æºè½‰å‹å‹"
        elif 'å¤ªç©ºç§‘æŠ€' in sectors:
            return "å‰æ²¿ç§‘æŠ€å‹"
        elif sentiments.count('positive') > sentiments.count('negative'):
            return "æ¨‚è§€é€²å–å‹"
        else:
            return "ç©©å¥ä¿å®ˆå‹"


def main():
    """æ¸¬è©¦åˆ†æåŠŸèƒ½."""
    analyzer = ArticleAnalyzer()
    
    # åˆ†æç‰¹å®šæ–‡ç« 
    print("ğŸ” åˆ†æ mrp çš„æœ€æ–°æ–‡ç« ...")
    analysis = analyzer.analyze_article_by_url("https://www.ptt.cc/bbs/Stock/M.1759822323.A.E44.html")
    
    if "error" not in analysis:
        print(f"ğŸ“° æ–‡ç« : {analysis['title']}")
        print(f"ä½œè€…: {analysis['author']}")
        print(f"æ¨æ–‡æ•¸: {analysis['push_count']}")
        print()
        
        print("ğŸ“Š åˆ†æçµæœ:")
        print(f"è‚¡ç¥¨ä»£ç¢¼: {analysis['analysis']['stocks']['mentioned_stocks']}")
        print(f"æŠ•è³‡ç­–ç•¥: {analysis['analysis']['strategy']['strategy_type']}")
        print(f"æƒ…æ„Ÿå‚¾å‘: {analysis['analysis']['sentiment']['sentiment']}")
        print(f"ç”¢æ¥­é¡åˆ¥: {analysis['analysis']['sectors']}")
        print(f"æŠ•è³‡å»ºè­°: {len(analysis['analysis']['recommendations'])} é …")
        print(f"é¢¨éšªæç¤º: {analysis['analysis']['risks']}")
        print(f"æŠ•è³‡è«–è¿°: {analysis['analysis']['investment_thesis']}")
        print(f"æ™‚é–“æ¡†æ¶: {analysis['analysis']['time_horizon']}")
    
    # åˆ†æä½œè€…æŠ•è³‡åå¥½
    print("\nğŸ‘¤ åˆ†æ mrp çš„æŠ•è³‡åå¥½...")
    profile = analyzer.get_author_investment_profile("mrp")
    
    if "error" not in profile:
        print(f"ç¸½æ–‡ç« æ•¸: {profile['total_articles']}")
        print(f"åå¥½è‚¡ç¥¨: {profile['favorite_stocks'][:5]}")
        print(f"åå¥½ç”¢æ¥­: {profile['favorite_sectors']}")
        print(f"æƒ…æ„Ÿåˆ†å¸ƒ: {profile['sentiment_distribution']}")
        print(f"æŠ•è³‡é¢¨æ ¼: {profile['investment_style']}")


if __name__ == "__main__":
    main()
