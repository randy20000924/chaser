#!/usr/bin/env python3
"""æ–‡ç« åˆ†ææ¨¡çµ„ - åˆ†æPTTæ–‡ç« ä¸­çš„æŠ•è³‡æ¨™çš„å’Œç­–ç•¥ï¼Œä¸¦å°‡çµæœå­˜å„²åˆ°è³‡æ–™åº«."""

import re
import json
from typing import List, Dict, Optional, Tuple
import httpx
from datetime import datetime
from loguru import logger
from database import db_manager
from models import PTTArticle
from sqlalchemy import desc


class ArticleAnalyzer:
    """æ–‡ç« åˆ†æå™¨ - ç”¨æ–¼çˆ¬èŸ²å¾Œè™•ç†å’Œå³æ™‚åˆ†æ."""
    
    def __init__(self):
        self.stock_patterns = {
            # ç¾è‚¡ä»£ç¢¼ (1-5å€‹å­—æ¯)
            'us_stocks': r'\b([A-Z]{1,5})\b',
            # å°è‚¡ä»£ç¢¼ (4ä½æ•¸å­—)
            'tw_stocks': r'\b([0-9]{4})\b',
            # æ¸¯è‚¡ä»£ç¢¼ (4-5ä½æ•¸å­—)
            'hk_stocks': r'\b([0-9]{4,5})\b'
        }
        
        # æŠ•è³‡ç­–ç•¥é—œéµè©
        self.strategy_keywords = {
            'buy_signals': ['è²·', 'å¤š', 'çœ‹å¤š', 'æ¨è–¦', 'å»ºè­°', 'å»ºå€‰', 'åŠ å€‰', 'æŒæœ‰'],
            'sell_signals': ['è³£', 'ç©º', 'çœ‹ç©º', 'æ¸›å€‰', 'å‡ºå ´', 'ç²åˆ©äº†çµ'],
            'risk_keywords': ['é¢¨éšª', 'æ³¨æ„', 'å°å¿ƒ', 'è¬¹æ…', 'æ³¢å‹•', 'ä¸ç¢ºå®š'],
            'sector_keywords': ['åŠå°é«”', 'AI', 'é›»å‹•è»Š', 'æ–°èƒ½æº', 'ç”ŸæŠ€', 'é‡‘è', 'åœ°ç”¢']
        }
    
    def process_crawled_articles(self, limit: int = None) -> Dict:
        """è™•ç†å·²çˆ¬å–ä½†æœªåˆ†æçš„æ–‡ç« ."""
        logger.info("é–‹å§‹è™•ç†å·²çˆ¬å–çš„æ–‡ç« ...")
        
        with db_manager.get_session() as session:
            # æŸ¥è©¢æœªåˆ†æçš„æ–‡ç« ï¼ˆanalysis_result ç‚º NULL æˆ–ç©ºï¼‰
            query = session.query(PTTArticle).filter(
                (PTTArticle.analysis_result.is_(None)) | 
                (PTTArticle.analysis_result == "")
            )
            
            if limit:
                query = query.limit(limit)
            
            articles = query.all()
            total_articles = len(articles)
            
            if total_articles == 0:
                logger.info("æ²’æœ‰éœ€è¦è™•ç†çš„æ–‡ç« ")
                return {"processed": 0, "errors": 0}
            
            logger.info(f"æ‰¾åˆ° {total_articles} ç¯‡éœ€è¦è™•ç†çš„æ–‡ç« ")
            
            processed = 0
            errors = 0
            
            for article in articles:
                try:
                    # åˆ†ææ–‡ç« 
                    analysis_result = self._analyze_content(article)
                    
                    # å°‡åˆ†æçµæœå­˜å„²åˆ°è³‡æ–™åº«
                    article.analysis_result = json.dumps(analysis_result, ensure_ascii=False)
                    article.analysis_time = datetime.now()
                    
                    processed += 1
                    
                    if processed % 10 == 0:
                        logger.info(f"å·²è™•ç† {processed}/{total_articles} ç¯‡æ–‡ç« ")
                        
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ç«  {article.article_id} å¤±æ•—: {e}")
                    errors += 1
                    continue
            
            # æäº¤æ‰€æœ‰æ›´æ”¹
            session.commit()
            
            logger.info(f"æ–‡ç« è™•ç†å®Œæˆ: æˆåŠŸ {processed} ç¯‡, éŒ¯èª¤ {errors} ç¯‡")
            return {"processed": processed, "errors": errors}
    
    def get_article_analysis(self, article_id: str) -> Dict:
        """å¾è³‡æ–™åº«ç²å–æ–‡ç« åˆ†æçµæœ."""
        with db_manager.get_session() as session:
            article = session.query(PTTArticle).filter(
                PTTArticle.article_id == article_id
            ).first()
            
            if not article:
                return {"error": "æ–‡ç« ä¸å­˜åœ¨"}
            
            # å¦‚æœå·²æœ‰åˆ†æçµæœï¼Œç›´æ¥è¿”å›
            if article.analysis_result:
                try:
                    return json.loads(article.analysis_result)
                except json.JSONDecodeError:
                    logger.warning(f"æ–‡ç«  {article_id} çš„åˆ†æçµæœæ ¼å¼éŒ¯èª¤ï¼Œé‡æ–°åˆ†æ")
            
            # å¦‚æœæ²’æœ‰åˆ†æçµæœï¼Œé€²è¡Œå³æ™‚åˆ†æ
            logger.info(f"æ–‡ç«  {article_id} æ²’æœ‰åˆ†æçµæœï¼Œé€²è¡Œå³æ™‚åˆ†æ")
            analysis_result = self._analyze_content(article)
            
            # ä¿å­˜åˆ†æçµæœåˆ°è³‡æ–™åº«
            try:
                article.analysis_result = json.dumps(analysis_result, ensure_ascii=False)
                article.analysis_time = datetime.now()
                session.commit()
            except Exception as e:
                logger.error(f"ä¿å­˜åˆ†æçµæœå¤±æ•—: {e}")
            
            return analysis_result
    
    def get_author_articles_with_analysis(self, author: str, limit: int = 20) -> Dict:
        """ç²å–ä½œè€…çš„æ–‡ç« åŠå…¶åˆ†æçµæœ."""
        with db_manager.get_session() as session:
            articles = session.query(PTTArticle).filter(
                PTTArticle.author == author
            ).order_by(desc(PTTArticle.publish_time)).limit(limit).all()
            
            if not articles:
                return {"error": "æ‰¾ä¸åˆ°è©²ä½œè€…çš„æ–‡ç« "}
            
            result = []
            for article in articles:
                article_data = {
                    "article_id": article.article_id,
                    "title": article.title,
                    "author": article.author,
                    "publish_time": article.publish_time.isoformat() if article.publish_time else None,
                    "url": article.url,
                    "push_count": article.push_count,
                    "analysis": None
                }
                
                # å¦‚æœæœ‰åˆ†æçµæœï¼Œè§£æä¸¦æ·»åŠ 
                if article.analysis_result:
                    try:
                        article_data["analysis"] = json.loads(article.analysis_result)
                    except json.JSONDecodeError:
                        logger.warning(f"æ–‡ç«  {article.article_id} çš„åˆ†æçµæœæ ¼å¼éŒ¯èª¤")
                
                result.append(article_data)
            
            return {
                "author": author,
                "articles": result,
                "total": len(result)
            }
    
    def _analyze_content(self, article: PTTArticle) -> Dict:
        """åˆ†ææ–‡ç« å…§å®¹ - ç°¡åŒ–è¼¸å‡ºæ ¼å¼."""
        content = article.content
        title = article.title
        
        # è‹¥å•Ÿç”¨ Ollamaï¼Œå„ªå…ˆèµ° LLM åˆ†æè·¯å¾‘
        try:
            from config import settings
            if getattr(settings, "enable_ollama", False):
                llm = self._analyze_with_ollama(
                    content=content, 
                    author=article.author, 
                    url=article.url, 
                    date=article.publish_time
                )
                if isinstance(llm, dict) and llm.get("recommended_stocks"):
                    return llm
        except Exception as e:
            logger.warning(f"Ollama analysis failed or disabled, fallback to rules. Reason: {e}")
        
        # æå–è‚¡ç¥¨ä»£ç¢¼
        stocks = self._extract_stocks(content)
        
        # åˆ†ææŠ•è³‡ç­–ç•¥
        strategy = self._analyze_strategy(content)
        
        # åˆ†ææƒ…æ„Ÿå‚¾å‘
        sentiment = self._analyze_sentiment(content)
        
        # åˆ†æç”¢æ¥­é¡åˆ¥
        sectors = self._analyze_sectors(content)
        
        # åˆ†ææŠ•è³‡å»ºè­°
        recommendations = self._extract_recommendations(content)
        
        # åˆ†æé¢¨éšªæç¤º
        risks = self._extract_risks(content)
        
        # ç°¡åŒ–è¼¸å‡ºæ ¼å¼
        return {
            "author": article.author,
            "date": article.publish_time.strftime('%Y-%m-%d') if article.publish_time else 'N/A',
            "url": article.url,
            "recommended_stocks": list(stocks.get('mentioned_stocks', []))[:5],  # æœ€å¤š5å€‹æ¨è–¦æ¨™çš„
            "reason": self._generate_simple_reason(stocks, strategy, sentiment, sectors, risks)
        }

    def _analyze_with_ollama(self, content: str, author: str, url: str, date: Optional[datetime]) -> Dict:
        """ä½¿ç”¨æœ¬åœ° Ollama é€²è¡Œåˆ†æï¼Œè¿”å›æ—¢å®š JSON çµæ§‹ã€‚"""
        from config import settings
        base = settings.ollama_base_url.rstrip("/")
        model = settings.ollama_model
        prompt = (
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‚¡ç¥¨åˆ†æå¸«ã€‚è«‹åˆ†æä»¥ä¸‹PTTè‚¡ç¥¨ç‰ˆæ–‡ç« ï¼Œæå–æŠ•è³‡æ¨™çš„å’Œæ¨è–¦åŸå› ã€‚\n\n"
            "åˆ†æé‡é»ï¼š\n"
            "1. æ‰¾å‡ºæ–‡ç« æ¨è–¦çš„è‚¡ç¥¨ä»£ç¢¼ï¼ˆå°è‚¡ç‚ºæ•¸å­—ä»£ç¢¼ä¸¦é™„å¸¶å…¬å¸åç¨±ï¼Œå¦‚å°ç©é›»2330ï¼Œç¾è‚¡ç›´æ¥ä½¿ç”¨ä»£ç¢¼ï¼Œå¦‚AAPLï¼‰\n"
            "2. åˆ†ææ¨è–¦åŸå› ï¼ˆæŠ€è¡“é¢ã€åŸºæœ¬é¢ã€æ¶ˆæ¯é¢ç­‰ï¼‰\n"
            "3. åˆ¤æ–·æŠ•è³‡æ–¹å‘ï¼ˆçœ‹å¤š/çœ‹ç©ºï¼‰\n\n"
            f"æ–‡ç« è³‡è¨Šï¼š\n"
            f"ä½œè€…: {author}\n"
            f"URL: {url}\n"
            f"æ—¥æœŸ: {(date.strftime('%Y-%m-%d') if isinstance(date, datetime) else 'N/A')}\n\n"
            f"æ–‡ç« å…§å®¹ï¼š\n{content}\n\n"
            "è«‹è¼¸å‡ºJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š\n"
            "- author: ä½œè€…åç¨±\n"
            "- date: ç™¼æ–‡æ—¥æœŸ (YYYY-MM-DD)\n"
            "- url: æ–‡ç« é€£çµ\n"
            "- recommended_stocks: æ¨è–¦çš„è‚¡ç¥¨ä»£ç¢¼é™£åˆ—ï¼ˆæœ€å¤š5å€‹ï¼‰\n"
            "- reason: æ¨è–¦åŸå› ï¼ˆç°¡æ½”èªªæ˜ï¼Œå¦‚ï¼šæŠ€è¡“é¢çªç ´ã€åŸºæœ¬é¢æ”¹å–„ã€æ¶ˆæ¯é¢åˆ©å¤šç­‰ï¼‰\n\n"
            "åªè¼¸å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"
        )
        payload = {"model": model, "prompt": prompt, "stream": False}
        try:
            # æé«˜è¶…æ™‚ï¼Œä¸¦é‡è©¦ä¸€æ¬¡
            with httpx.Client(timeout=httpx.Timeout(60.0)) as client:
                resp = client.post(f"{base}/api/generate", json=payload)
                resp.raise_for_status()
                data = resp.json()
                # Ollama å›å‚³å½¢å¦‚ {"response": "...æ–‡æœ¬..."}
                raw = data.get("response", "").strip()
                # å˜—è©¦è§£æJSON
                result = json.loads(raw)
                # å¾Œè™•ç†ï¼šç¢ºä¿ recommended_stocks åªåŒ…å«æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç¢¼
                stocks = result.get("recommended_stocks", [])
                valid_stocks = []
                for stock in stocks:
                    if isinstance(stock, str):
                        # å°è‚¡4ä½æ•¸ä»£ç¢¼
                        if re.match(r'^\d{4}$', stock):
                            valid_stocks.append(stock)
                        # ç¾è‚¡è‹±æ–‡å­—æ¯ä»£ç¢¼ (1-5å€‹å­—æ¯)
                        elif re.match(r'^[A-Z]{1,5}$', stock):
                            valid_stocks.append(stock)
                
                # æ­£è¦åŒ–æ¬„ä½
                return {
                    "author": result.get("author", author),
                    "date": result.get("date", (date.strftime('%Y-%m-%d') if isinstance(date, datetime) else 'N/A')),
                    "url": result.get("url", url),
                    "recommended_stocks": valid_stocks[:5],
                    "reason": result.get("reason", "")
                }
        except Exception as e:
            logger.warning(f"Ollama call/parse error: {e}")
            return {}
    
    def _generate_simple_reason(self, stocks: Dict, strategy: Dict, sentiment: Dict, sectors: List, risks: List) -> str:
        """ç”Ÿæˆç°¡åŒ–çš„æ¨è–¦åŸå› ."""
        reasons = []
        
        # åŸºæ–¼è‚¡ç¥¨ä»£ç¢¼æ•¸é‡
        stock_count = len(stocks.get('mentioned_stocks', []))
        if stock_count > 0:
            if stock_count == 1:
                reasons.append(f"æ¨è–¦æ¨™çš„: {stocks['mentioned_stocks'][0]}")
            else:
                reasons.append(f"æ¨è–¦{stock_count}å€‹æ¨™çš„")
        
        # åŸºæ–¼ç­–ç•¥
        buy_signals = len(strategy.get('buy_signals', []))
        sell_signals = len(strategy.get('sell_signals', []))
        
        if buy_signals > sell_signals:
            reasons.append("çœ‹å¤šè¨Šè™Ÿ")
        elif sell_signals > buy_signals:
            reasons.append("çœ‹ç©ºè¨Šè™Ÿ")
        
        # åŸºæ–¼æƒ…æ„Ÿ
        positive = sentiment.get('positive', 0)
        negative = sentiment.get('negative', 0)
        
        if positive > negative:
            reasons.append("æ­£é¢æƒ…ç·’")
        elif negative > positive:
            reasons.append("è² é¢æƒ…ç·’")
        
        # åŸºæ–¼ç”¢æ¥­
        if sectors:
            reasons.append(f"é—œæ³¨{sectors[0]}ç”¢æ¥­")
        
        # åŸºæ–¼é¢¨éšª
        if risks:
            reasons.append("æ³¨æ„é¢¨éšª")
        
        # å¦‚æœæ²’æœ‰å…·é«”åŸå› ï¼Œå˜—è©¦å¾å…§å®¹ä¸­æå–é—œéµä¿¡æ¯
        if not reasons:
            reasons.append("æŠ€è¡“åˆ†æ")
        
        return "ã€".join(reasons) if reasons else "æŠ€è¡“åˆ†æ"
    
    def _extract_stocks(self, content: str) -> Dict:
        """æå–è‚¡ç¥¨ä»£ç¢¼."""
        stocks = {
            "us_stocks": [],
            "tw_stocks": [],
            "hk_stocks": [],
            "mentioned_stocks": []
        }
        
        # æ”¹é€²çš„å°è‚¡ä»£ç¢¼æå– - 4ä½æ•¸å­—ï¼Œæ’é™¤å¹´ä»½å’Œå¸¸è¦‹æ•¸å­—
        tw_pattern = r'\b([0-9]{4})\b'
        tw_matches = re.findall(tw_pattern, content)
        
        # éæ¿¾æ‰å¹´ä»½å’Œå¸¸è¦‹æ•¸å­—
        filtered_tw = []
        for match in tw_matches:
            # æ’é™¤å¹´ä»½ (2020-2030)
            if not (2020 <= int(match) <= 2030):
                # æ’é™¤å¸¸è¦‹çš„æ•¸å­—åºåˆ—å’ŒæŠ€è¡“è¦æ ¼
                if not match in ['0000', '1111', '2222', '3333', '4444', '5555', '6666', '7777', '8888', '9999', '3120', '2500', '1230', '1070', '1090', '1330', '1575', '8000', '5800', '3000', '1000', '2000', '10000', '8550', '1400', '2800', '5600']:
                    filtered_tw.append(match)
        
        stocks["tw_stocks"] = list(set(filtered_tw))
        
        # æå–ç¾è‚¡ä»£ç¢¼ (1-5å€‹å­—æ¯)
        us_pattern = r'\b([A-Z]{1,5})\b'
        us_matches = re.findall(us_pattern, content)
        # éæ¿¾æ‰å¸¸è¦‹çš„è‹±æ–‡å–®è©å’ŒæŠ€è¡“è¡“èª
        common_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'HER', 'WAS', 'ONE', 'OUR', 'HAD', 'BY', 'WORD', 'BUT', 'WHAT', 'SOME', 'WE', 'IT', 'IS', 'OR', 'HAD', 'THE', 'OF', 'TO', 'AND', 'A', 'IN', 'IS', 'IT', 'YOU', 'THAT', 'HE', 'WAS', 'FOR', 'ON', 'ARE', 'AS', 'WITH', 'HIS', 'THEY', 'I', 'AT', 'BE', 'THIS', 'HAVE', 'FROM', 'OR', 'ONE', 'HAD', 'BY', 'WORD', 'BUT', 'NOT', 'WHAT', 'ALL', 'WERE', 'WE', 'WHEN', 'YOUR', 'CAN', 'SAID', 'THERE', 'EACH', 'WHICH', 'SHE', 'DO', 'HOW', 'THEIR', 'IF', 'WILL', 'UP', 'OTHER', 'ABOUT', 'OUT', 'MANY', 'THEN', 'THEM', 'THESE', 'SO', 'SOME', 'HER', 'WOULD', 'MAKE', 'LIKE', 'INTO', 'HIM', 'TIME', 'HAS', 'TWO', 'MORE', 'GO', 'NO', 'WAY', 'COULD', 'MY', 'THAN', 'FIRST', 'BEEN', 'CALL', 'WHO', 'ITS', 'NOW', 'FIND', 'LONG', 'DOWN', 'DAY', 'DID', 'GET', 'COME', 'MADE', 'MAY', 'PART', 'ATH', 'Q4', 'Q1', 'Q2', 'EPS', 'PE', 'DJI', 'AI', 'COBRA', 'RKLB', 'ONDS', 'AVAV', 'RCAT', 'PDYN', 'TOP', 'M', 'W', 'X', 'Y', 'Z'}
        stocks["us_stocks"] = list(set([match for match in us_matches if match not in common_words and len(match) <= 5]))
        
        # æå–æ¸¯è‚¡ä»£ç¢¼ (4-5ä½æ•¸å­—)
        hk_pattern = r'\b([0-9]{4,5})\b'
        hk_matches = re.findall(hk_pattern, content)
        # éæ¿¾æ‰æŠ€è¡“è¦æ ¼æ•¸å­—
        filtered_hk = []
        for match in hk_matches:
            if len(match) >= 4 and not (2020 <= int(match) <= 2030):
                if not match in ['3120', '2500', '1230', '1070', '1090', '1330', '1575', '8000', '5800', '3000', '1000', '2000', '10000', '8550', '1400', '2800', '5600']:
                    filtered_hk.append(match)
        stocks["hk_stocks"] = list(set(filtered_hk))
        
        # åˆä½µæ‰€æœ‰è‚¡ç¥¨
        all_stocks = stocks["us_stocks"] + stocks["tw_stocks"] + stocks["hk_stocks"]
        stocks["mentioned_stocks"] = list(set(all_stocks))
        
        return stocks
    
    def _analyze_strategy(self, content: str) -> Dict:
        """åˆ†ææŠ•è³‡ç­–ç•¥."""
        strategy = {
            "buy_signals": [],
            "sell_signals": [],
            "strategy_type": "unknown"
        }
        
        # æª¢æŸ¥è²·å…¥ä¿¡è™Ÿ
        for keyword in self.strategy_keywords['buy_signals']:
            if keyword in content:
                strategy["buy_signals"].append(keyword)
        
        # æª¢æŸ¥è³£å‡ºä¿¡è™Ÿ
        for keyword in self.strategy_keywords['sell_signals']:
            if keyword in content:
                strategy["sell_signals"].append(keyword)
        
        # åˆ¤æ–·ç­–ç•¥é¡å‹
        if strategy["buy_signals"] and not strategy["sell_signals"]:
            strategy["strategy_type"] = "bullish"
        elif strategy["sell_signals"] and not strategy["buy_signals"]:
            strategy["strategy_type"] = "bearish"
        elif strategy["buy_signals"] and strategy["sell_signals"]:
            strategy["strategy_type"] = "mixed"
        
        return strategy
    
    def _analyze_sentiment(self, content: str) -> Dict:
        """åˆ†ææƒ…æ„Ÿå‚¾å‘."""
        positive_words = ['çœ‹å¥½', 'æ¨‚è§€', 'æˆé•·', 'æ©Ÿæœƒ', 'æ½›åŠ›', 'åˆ©å¤š', 'ä¸Šæ¼²', 'çªç ´']
        negative_words = ['çœ‹å£', 'æ‚²è§€', 'è¡°é€€', 'é¢¨éšª', 'åˆ©ç©º', 'ä¸‹è·Œ', 'ç ´åº•']
        
        positive_count = sum(1 for word in positive_words if word in content)
        negative_count = sum(1 for word in negative_words if word in content)
        
        if positive_count > negative_count:
            sentiment = "positive"
        elif negative_count > positive_count:
            sentiment = "negative"
        else:
            sentiment = "neutral"
        
        return {
            "sentiment": sentiment,
            "positive_score": positive_count,
            "negative_score": negative_count,
            "confidence": abs(positive_count - negative_count) / max(positive_count + negative_count, 1)
        }
    
    def _analyze_sectors(self, content: str) -> List[str]:
        """åˆ†æç”¢æ¥­é¡åˆ¥."""
        sectors = []
        for sector in self.strategy_keywords['sector_keywords']:
            if sector in content:
                sectors.append(sector)
        
        # é¡å¤–çš„ç”¢æ¥­è­˜åˆ¥
        if 'åŠå°é«”' in content or 'æ™¶ç‰‡' in content or 'AMD' in content or 'NV' in content:
            sectors.append('åŠå°é«”')
        if 'AI' in content or 'äººå·¥æ™ºæ…§' in content:
            sectors.append('AI')
        if 'é›»å‹•è»Š' in content or 'ç‰¹æ–¯æ‹‰' in content or 'TSLA' in content:
            sectors.append('é›»å‹•è»Š')
        if 'æ–°èƒ½æº' in content or 'å¤ªé™½èƒ½' in content or 'é¢¨é›»' in content:
            sectors.append('æ–°èƒ½æº')
        if 'ç”ŸæŠ€' in content or 'é†«ç™‚' in content or 'è£½è—¥' in content:
            sectors.append('ç”ŸæŠ€')
        if 'è¡›æ˜Ÿ' in content or 'å¤ªç©º' in content or 'SpaceX' in content:
            sectors.append('å¤ªç©ºç§‘æŠ€')
        if 'æ ¸èƒ½' in content or 'æ ¸é›»' in content:
            sectors.append('æ ¸èƒ½')
        
        return list(set(sectors))
    
    def _extract_recommendations(self, content: str) -> List[Dict]:
        """æå–æŠ•è³‡å»ºè­°."""
        recommendations = []
        
        # å°‹æ‰¾åƒ¹æ ¼å»ºè­°
        price_patterns = [
            r'(\d+(?:\.\d+)?)\s*å…ƒ',
            r'(\d+(?:\.\d+)?)\s*ç¾å…ƒ',
            r'(\d+(?:\.\d+)?)\s*USD'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                recommendations.append({
                    "type": "price_target",
                    "value": float(match),
                    "context": "åƒ¹æ ¼ç›®æ¨™"
                })
        
        # å°‹æ‰¾æ™‚é–“å»ºè­°
        time_patterns = [
            r'(\d+)\s*å¹´',
            r'(\d+)\s*æœˆ',
            r'(\d+)\s*å­£'
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                recommendations.append({
                    "type": "time_horizon",
                    "value": int(match),
                    "context": "æŠ•è³‡æœŸé–“"
                })
        
        return recommendations
    
    def _extract_risks(self, content: str) -> List[str]:
        """æå–é¢¨éšªæç¤º."""
        risks = []
        
        risk_indicators = [
            'é¢¨éšª', 'æ³¨æ„', 'å°å¿ƒ', 'è¬¹æ…', 'æ³¢å‹•', 'ä¸ç¢ºå®š', 'å¯èƒ½', 'æˆ–è¨±',
            'ä½†æ˜¯', 'ä¸é', 'ç„¶è€Œ', 'é›–ç„¶', 'å„˜ç®¡'
        ]
        
        for indicator in risk_indicators:
            if indicator in content:
                risks.append(indicator)
        
        return list(set(risks))
    
    def get_analysis_statistics(self) -> Dict:
        """ç²å–åˆ†æçµ±è¨ˆä¿¡æ¯."""
        with db_manager.get_session() as session:
            total_articles = session.query(PTTArticle).count()
            analyzed_articles = session.query(PTTArticle).filter(
                PTTArticle.analysis_result.isnot(None),
                PTTArticle.analysis_result != ""
            ).count()
            
            return {
                "total_articles": total_articles,
                "analyzed_articles": analyzed_articles,
                "pending_articles": total_articles - analyzed_articles,
                "analysis_rate": f"{(analyzed_articles / total_articles * 100):.1f}%" if total_articles > 0 else "0%"
            }


def main():
    """æ¸¬è©¦åˆ†æåŠŸèƒ½."""
    analyzer = ArticleAnalyzer()
    
    # æ¸¬è©¦è™•ç†å·²çˆ¬å–çš„æ–‡ç« 
    print("ğŸ” é–‹å§‹è™•ç†å·²çˆ¬å–çš„æ–‡ç« ...")
    result = analyzer.process_crawled_articles(limit=5)
    print(f"è™•ç†çµæœ: {result}")
    
    # æ¸¬è©¦ç²å–åˆ†æçµ±è¨ˆ
    print("\nğŸ“Š åˆ†æçµ±è¨ˆ:")
    stats = analyzer.get_analysis_statistics()
    print(f"ç¸½æ–‡ç« æ•¸: {stats['total_articles']}")
    print(f"å·²åˆ†æ: {stats['analyzed_articles']}")
    print(f"å¾…åˆ†æ: {stats['pending_articles']}")
    print(f"åˆ†æç‡: {stats['analysis_rate']}")


if __name__ == "__main__":
    main()